{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPG8s8nnJbc9/zy+lxFL54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlichter2/cesium/blob/master/colab_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpByFFPnOrlg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "037f2ab8-37f9-4843-d8c2-8a4a4c0cbd83"
      },
      "source": [
        "!pip install rasterio\n",
        "!pip install geopandas"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rasterio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/7e/eed7dfd109fc89ed3cf8b5ed3f26f841b03b92f6ca1c31c4745f938a081b/rasterio-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (18.2MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2MB 228kB/s \n",
            "\u001b[?25hCollecting click-plugins\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Collecting affine\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (19.3.0)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "Installing collected packages: click-plugins, affine, snuggs, cligj, rasterio\n",
            "Successfully installed affine-2.3.0 click-plugins-1.1.1 cligj-0.5.0 rasterio-1.1.5 snuggs-1.4.7\n",
            "Collecting geopandas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a4/e66aafbefcbb717813bf3a355c8c4fc3ed04ea1dd7feb2920f2f4f868921/geopandas-0.8.1-py2.py3-none-any.whl (962kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 9.0MB/s \n",
            "\u001b[?25hCollecting pyproj>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c3/071e080230ac4b6c64f1a2e2f9161c9737a2bc7b683d2c90b024825000c0/pyproj-2.6.1.post1-cp36-cp36m-manylinux2010_x86_64.whl (10.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9MB 30.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.0.5)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.7.0)\n",
            "Collecting fiona\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/20/4e63bc5c6e62df889297b382c3ccd4a7a488b00946aaaf81a118158c6f09/Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 251kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.1.2)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (19.3.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (0.5.0)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.12.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.1.1)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Installing collected packages: pyproj, munch, fiona, geopandas\n",
            "Successfully installed fiona-1.8.13.post1 geopandas-0.8.1 munch-2.5.0 pyproj-2.6.1.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaeddZOMoOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import imageio\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\n",
        "from tensorflow.keras.losses import binary_crossentropy, mean_squared_error, kullback_leibler_divergence, \\\n",
        "    mean_absolute_error, cosine_similarity\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.initializers import RandomNormal, GlorotNormal, Zeros\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import rasterio\n",
        "import geopandas as gpd\n",
        "import datetime\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "IMAGE_DIM_X = 100\n",
        "IMAGE_DIM_Y = 300\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E06le3OQOOs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# def custom_loss(y_true, y_pred):\n",
        "#\n",
        "#   bce = binary_crossentropy(y_true[:, :-1], y_pred[:, :-1])\n",
        "#   bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "#   return 2 * bce * y_true[:, -1] + 0.5 * bce2\n",
        "\n",
        "def custom_loss_bce(y_true, y_pred):\n",
        "    # bce = binary_crossentropy(y_true[:, :-1], y_pred[:, :-1])\n",
        "    l1 = binary_crossentropy(y_true[:, :25], y_pred[:, :25])\n",
        "    l2 = binary_crossentropy(y_true[:,75 :100], y_pred[:,75 :100])\n",
        "    l3 = binary_crossentropy(y_true[:, 100:125], y_pred[:, 100:125])\n",
        "    l4 = binary_crossentropy(y_true[:, 175:200], y_pred[:, 175:200])\n",
        "    l5 = binary_crossentropy(y_true[:, 25:75], y_pred[:, 25:75])\n",
        "    l6 = binary_crossentropy(y_true[:, 125:175], y_pred[:, 125:175])\n",
        "    bce = binary_crossentropy(y_true[:, 200:-1], y_pred[:, 200:-1])\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 2 * ((l1+l2+l3+l4+l5+l6)/6) + 1 * bce2+2*(bce*y_true[:, -1])\n",
        "\n",
        "def custom_loss_bce_simple(y_true, y_pred):\n",
        "    l1 = binary_crossentropy(y_true[:, :200], y_pred[:, :200])\n",
        "    bce = binary_crossentropy(y_true[:,200 :-1], y_pred[:,200 :-1])\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 2* l1 + 1 * bce2+2*(bce*y_true[:, -1])\n",
        "\n",
        "\n",
        "def custom_loss_combined(y_true, y_pred):\n",
        "    # bce = binary_crossentropy(y_true[:, :-1], y_pred[:, :-1])\n",
        "    l1 = binary_crossentropy(y_true[:, :25], y_pred[:, :25])\n",
        "    l2 = binary_crossentropy(y_true[:, 75:100], y_pred[:, 75:100])\n",
        "    l3 = binary_crossentropy(y_true[:, 100:125], y_pred[:, 100:125])\n",
        "    l4 = binary_crossentropy(y_true[:, 175:200], y_pred[:, 175:200])\n",
        "    l5 = binary_crossentropy(y_true[:, 25:75], y_pred[:, 25:75])\n",
        "    l6 = binary_crossentropy(y_true[:, 125:175], y_pred[:, 125:175])\n",
        "\n",
        "\n",
        "    l11 = mean_absolute_error(y_true[:, :25], y_pred[:, :25])\n",
        "    l22 = mean_absolute_error(y_true[:,75 :100], y_pred[:,75 :100])\n",
        "    l33 = mean_absolute_error(y_true[:, 100:125], y_pred[:, 100:125])\n",
        "    l44 = mean_absolute_error(y_true[:, 175:200], y_pred[:, 175:200])\n",
        "    l55 = mean_absolute_error(y_true[:, 25:75], y_pred[:, 25:75])\n",
        "    l66 = mean_absolute_error(y_true[:, 125:175], y_pred[:, 125:175])\n",
        "\n",
        "    bce = binary_crossentropy(y_true[:,200 :-1], y_pred[:,200 :-1])\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 1 * ((l1+l2+l3+l4+l5+l6)/6) +3 * ((l11+l22+l33+l44+l55+l66)/6) + 1 * bce2+2*(bce*y_true[:, -1])\n",
        "\n",
        "\n",
        "def custom_loss_mse(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true[:, :-1], y_pred[:, :-1])\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 0.99 * mse + 0.1 * bce2\n",
        "\n",
        "\n",
        "def custom_loss_mae(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true[:, :-1], y_pred[:, :-1])\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 0.99 * mae + 0.1 * bce2\n",
        "\n",
        "def custom_loss_css(y_true, y_pred):\n",
        "    css1 = cosine_similarity(y_true[:, :25], y_pred[:, :25])\n",
        "    css2 = cosine_similarity(y_true[:,75 :100], y_pred[:,75 :100])\n",
        "    css3 = cosine_similarity(y_true[:, 100:125], y_pred[:, 100:125])\n",
        "    css4 = cosine_similarity(y_true[:, 175:200], y_pred[:, 175:200])\n",
        "    css5 = cosine_similarity(y_true[:, 25:75], y_pred[:, 25:75])\n",
        "    css6 = cosine_similarity(y_true[:, 125:175], y_pred[:, 125:175])\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 0.99 * ((css1+css2+css3+css4+css5+css6)/6) + 0.1 * bce2\n",
        "\n",
        "\n",
        "def custom_loss_kld(y_true, y_pred):\n",
        "    kld = kullback_leibler_divergence(y_true[:, :-1], y_pred[:, :-1])\n",
        "    # kld = tf.reduce_sum(kld , axis=0)\n",
        "    # kld = tf.reduce_mean(kld )\n",
        "    bce2 = binary_crossentropy(y_true[:, -1], y_pred[:, -1])\n",
        "    return 0.00099 * kld + 0.1 * bce2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_model(network='vgg16',\n",
        "               weights='imagenet',\n",
        "               loss='bce',\n",
        "               activation='sigmoid',\n",
        "               optimizer='Adam',\n",
        "               lr=0.001,\n",
        "               width=100,\n",
        "               height=100):\n",
        "    if network == 'resnet50':\n",
        "        Net = tf.keras.applications.ResNet50\n",
        "    else:\n",
        "        Net = tf.keras.applications.VGG16\n",
        "\n",
        "    if loss == 'mse':\n",
        "        loss = custom_loss_mse\n",
        "    elif loss == 'mae':\n",
        "        loss = custom_loss_mae\n",
        "    elif loss == 'kld':\n",
        "        loss = custom_loss_kld\n",
        "    elif loss =='css':\n",
        "        loss = cosine_similarity\n",
        "    elif loss=='comb':\n",
        "        loss = custom_loss_combined\n",
        "    elif loss == 'bce_simp':\n",
        "        loss = custom_loss_bce_simple\n",
        "    else:\n",
        "        loss = custom_loss_bce\n",
        "\n",
        "    if optimizer == 'SGD':\n",
        "        optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
        "    else:\n",
        "        optimizer = Adam(lr=lr)\n",
        "\n",
        "\n",
        "    net = Net(\n",
        "        input_shape=[height, width, 3],\n",
        "        include_top=False,\n",
        "        weights=weights)\n",
        "    for l in net.layers:\n",
        "        l.trainable = True\n",
        "    x = Flatten()(net.output)\n",
        "    # x = Dense(1000, activation='relu')(x)\n",
        "    # x = Dropout(.6)(x)\n",
        "    # x = Dense(500, activation='relu')(x)\n",
        "    # x = Dropout(.6)(x)\n",
        "    x = Dense(width*2+5,\n",
        "              kernel_initializer=GlorotNormal(),\n",
        "              bias_initializer=Zeros(),\n",
        "              activation=activation)(x)\n",
        "    # x = tf.keras.activations.relu(x, alpha=0.0, max_value=1, threshold=0)\n",
        "    model = Model(net.input, x)\n",
        "    #   model.compile(loss=custom_loss, optimizer=SGD(learning_rate=0.01, momentum=0.9))\n",
        "    model.compile(loss=loss,\n",
        "                  run_eagerly=True,\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(\n",
        "                      name=\"mse\", dtype=None),\n",
        "                      tf.keras.metrics.RootMeanSquaredError(\n",
        "                          name=\"rmse\", dtype=None\n",
        "                      ),\n",
        "                      tf.keras.metrics.BinaryCrossentropy(\n",
        "                          name=\"bce\", dtype=None\n",
        "                      ),\n",
        "                      tf.keras.metrics.MeanAbsoluteError(\n",
        "                          name=\"mae\", dtype=None),\n",
        "                      # tf.keras.metrics.MeanAbsolutePercentageError(\n",
        "                      #     name=\"mape\", dtype=None),\n",
        "                      tf.keras.metrics.MeanSquaredLogarithmicError(\n",
        "                          name=\"msle\", dtype=None\n",
        "                      ),\n",
        "                      tf.keras.metrics.CosineSimilarity(\n",
        "                          name=\"css\", dtype=None, axis=-1\n",
        "                      )\n",
        "                  ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def read_img(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "\n",
        "# def read_coords(coords, height):\n",
        "#     coords = np.array(json.loads(coords))\n",
        "#     coords[100:] = np.where((coords[:100] >= height - 1) | (coords[:100] <= 1), 0, coords[100:])\n",
        "#     coords[:100] = np.where((coords[:100] >= height - 1) | (coords[:100] <= 1), height / 2, coords[:100])\n",
        "#     return coords / height\n",
        "\n",
        "def read_coords(coords, height, width):\n",
        "    coords = np.array(json.loads(coords))\n",
        "    coords[width:] = np.where((coords[:width] >= height - 1) | (coords[:width] <= 1), 0, coords[width:])\n",
        "    coords[:width] = np.where((coords[:width] >= height - 1) | (coords[:width] <= 1), height / 2, coords[:width])\n",
        "\n",
        "    xs = np.where(coords[width:-1] > 0)\n",
        "    if xs[0].shape[0] > 0:\n",
        "        ymin = coords[:width].min() / height\n",
        "        ymax = (coords[:width] + coords[width:]).max() / height\n",
        "        xmin = xs[0].min() / width\n",
        "        xmax = xs[0].max() / width\n",
        "        box = np.array((ymin, xmin, ymax - ymin, xmax - xmin))\n",
        "    else:\n",
        "        box = np.zeros((4))\n",
        "\n",
        "    return np.append(coords / height, box)\n",
        "\n",
        "\n",
        "def img_generator(df, height, width, batch_size=64, network='vgg16'):\n",
        "    # generate image and targets\n",
        "    if network == 'resnet50':\n",
        "        preprocess_input = preprocess_input_resnet50\n",
        "    else:\n",
        "        preprocess_input = preprocess_input_vgg16\n",
        "    while True:\n",
        "        # Each epoch will have 50 batches. Why? No reason\n",
        "        for _ in range(50):\n",
        "            # ids = [np.random.randint(0, len(df)) for i in range(batch_size)]\n",
        "            # print(ids)\n",
        "            df = df.reset_index(drop = True)\n",
        "            sample = df.sample(n=batch_size).copy()\n",
        "            paths = sample['path'].tolist()\n",
        "\n",
        "            X = [read_img(path) for path in paths]\n",
        "            X = np.stack(X)\n",
        "            X = preprocess_input(X)\n",
        "            labels = sample['class'].tolist()\n",
        "            coords = sample['coords'].tolist()\n",
        "            Y = [read_coords(coord, height, width) for coord in coords]\n",
        "            Y = np.stack(Y)\n",
        "            Y = np.hstack([Y, np.array(labels).reshape(-1, 1)])\n",
        "\n",
        "            yield X, Y\n",
        "\n",
        "\n",
        "# def read_coords(path):\n",
        "#     with rasterio.open(path) as src:\n",
        "#         meta = src.meta\n",
        "#\n",
        "#     line = gpd.GeoDataFrame.from_file(path.replace('images/', 'lines/mask_').replace('tif', 'geojson'))\n",
        "#     coords = line.iloc[0]['geometry'].coords.xy\n",
        "#     rows, cols = rasterio.transform.rowcol(meta['transform'] * rasterio.Affine.translation(0, 100),\n",
        "#                                            coords[0],\n",
        "#                                            coords[1])\n",
        "#     rows = np.array(rows)\n",
        "#     rows = np.where(rows >= 99, 0, rows)\n",
        "#     return rows / 100\n",
        "\n",
        "\n",
        "# def img_generator(paths, batch_size=64):\n",
        "#     # generate image and targets\n",
        "#     while True:\n",
        "#         # Each epoch will have 50 batches. Why? No reason\n",
        "#         for _ in range(50):\n",
        "#             ids = [np.random.randint(0, paths.shape[0]) for i in range(batch_size)]\n",
        "#\n",
        "#             paths_list = paths[ids].copy().tolist()\n",
        "#\n",
        "#             X = [read_img(path) for path in paths_list]\n",
        "#             Y = np.array([read_coords(path) for path in paths_list])\n",
        "#             X = np.stack(X)\n",
        "#\n",
        "#             labels = np.where(Y[:, 49] == 0, 0, 1)\n",
        "#             Y = np.stack(Y)\n",
        "#             Y = np.hstack([Y, np.array(labels).reshape(-1, 1)])\n",
        "#\n",
        "#             yield X, Y\n",
        "\n",
        "def main():\n",
        "    data_path = '/media/ml/ex4/seetree/seetree_data/polygons_dataset/tree_in_center_w100_h300'\n",
        "    train = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "    val = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
        "\n",
        "    models_dir = 'models'\n",
        "\n",
        "    params = {\n",
        "        'ex': '205-bce',\n",
        "        'net': 'resnet50',\n",
        "        'lr': 0.0001,\n",
        "        'wei': 'imagenet',\n",
        "        'loss': 'bce_simp',\n",
        "        'act': 'sigmoid',\n",
        "        'opt': 'Adam',\n",
        "        'w': 100,\n",
        "        'h': 300,\n",
        "        'bs': 32\n",
        "\n",
        "    }\n",
        "    s = '{}'\n",
        "    for i in range(len(params.keys()) - 1):\n",
        "        s += '-{}'\n",
        "    logs_dir = 'logs' + '/' + s.format(*[i + '-' + str(j) for i, j in zip(params.keys(), params.values())])\n",
        "    csv_dir = 'csvs' + '/' + s.format(*[i + '-' + str(j) for i, j in zip(params.keys(), params.values())])\n",
        "    os.makedirs(logs_dir, exist_ok=False)\n",
        "    os.makedirs(csv_dir, exist_ok=False)\n",
        "    best_model_file = \\\n",
        "        '{}/'.format(models_dir) + 'epoch-{epoch:d}-' + s.format(\n",
        "            *[i + '-' + str(j) for i, j in zip(params.keys(), params.values())]) + \\\n",
        "        '-loss-{val_loss:0.7f}-mse-{val_mse:0.7f}-rmse-{val_rmse:0.7f}-bce-{val_bce:0.7f}' + \\\n",
        "        '-mae-{val_mae:0.7f}-msle-{val_msle:0.7f}-css-{val_css:0.7f}' + \\\n",
        "        '.h5'\n",
        "\n",
        "    model = make_model(params['net'],\n",
        "                       params['wei'],\n",
        "                       params['loss'],\n",
        "                       params['act'],\n",
        "                       params['opt'],\n",
        "                       params['lr'],\n",
        "                       params['w'],\n",
        "                       params['h'])\n",
        "    # model.load_weights(\n",
        "    #     '/media/ml/ex4/seetree/seetree_dev/scratch/polygon_prediction/models/resnet50-22-loss-0.6111830-mse-0.0479484-rmse-0.2189712-mae-0.1051554-mape-80671456.0000000-msle-0.0274004-css-0.6719598.h5')\n",
        "\n",
        "    best_model = ModelCheckpoint(best_model_file, monitor='val_loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 save_weights_only=True,\n",
        "                                 mode='min')\n",
        "    csv_logger = CSVLogger(os.path.join(csv_dir, 'training.log'))\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=logs_dir, histogram_freq=1, write_graph=True, write_images=True, update_freq=10)\n",
        "\n",
        "    callbacks = [best_model,\n",
        "                 csv_logger,\n",
        "                 tensorboard\n",
        "                 ]\n",
        "\n",
        "    r = model.fit(\n",
        "        img_generator(train,\n",
        "                      params['h'],\n",
        "                      params['w'],\n",
        "                      params['bs'],\n",
        "                      params['net']),\n",
        "        validation_data=img_generator(val,\n",
        "                                      params['h'],\n",
        "                                      params['w'],\n",
        "                                      params['bs'],\n",
        "                                      params['net']),\n",
        "        steps_per_epoch=25,\n",
        "        validation_steps=12,\n",
        "        epochs=300,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    print(r.history.keys())\n",
        "\n",
        "    # plot some data\n",
        "    plt.plot(r.history['loss'], label='loss')\n",
        "    plt.plot(r.history['val_loss'], label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # accuracies\n",
        "    plt.plot(r.history['mse'], label='acc')\n",
        "    plt.plot(r.history['val_mse'], label='val_acc')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnbLshhnM1vO",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}